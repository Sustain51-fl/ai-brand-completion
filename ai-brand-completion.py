import streamlit as st
import pandas as pd
import openai
import requests
import base64
import re

# === Secrets ===
openai.api_key = st.secrets["openai_api_key"]
GOOGLE_API_KEY = st.secrets["google_api_key"]
GOOGLE_CX = st.secrets["google_cse_id"]
GITHUB_TOKEN = st.secrets["github_token"]
GITHUB_REPO = st.secrets["github_repo"]
EXCLUDE_PATH = st.secrets["exclude_path"]

client = openai.OpenAI(api_key=openai.api_key)

st.set_page_config(layout="wide")
st.title("ğŸ§  ãƒ¡ãƒ¼ã‚«ãƒ¼ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰è£œå®Œãƒ„ãƒ¼ãƒ«ï¼ˆå†å‡¦ç†é˜²æ­¢ï¼‹ãƒªã‚»ãƒƒãƒˆå¯¾å¿œï¼‰")

# === é™¤å¤–ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿ï¼ˆGitHubã‹ã‚‰rawã§ï¼‰
@st.cache_data
def load_exclude_list_from_github():
    url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/main/{EXCLUDE_PATH}"
    try:
        df = pd.read_csv(url)
        return df.iloc[:, 0].dropna().tolist()
    except Exception as e:
        st.warning(f"âŒ é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def google_search(query, exclude_domains):
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_API_KEY,
        "cx": GOOGLE_CX,
        "q": query,
        "num": 5,
        "lr": "lang_ja"
    }
    try:
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        data = res.json()
        results, urls = [], []
        for item in data.get("items", []):
            link = item.get("link", "")
            if any(x in link for x in exclude_domains):
                continue
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            results.append(f"{title}\n{snippet}\n{link}")
            urls.append(link)
        return "\n\n".join(results), ", ".join(urls[:3])
    except Exception as e:
        return f"[GoogleSearchError] {e}", ""

def get_safe(row, col):
    return row[col] if col in row and pd.notnull(row[col]) else ""

def parse_gpt_output(text):
    brand, maker, reason = "", "", ""
    for line in text.splitlines():
        b = re.match(r"^\s*(ãƒ–ãƒ©ãƒ³ãƒ‰å|ãƒ–ãƒ©ãƒ³ãƒ‰|ï¾Œï¾ï¾—ï¾ï¾„ï¾)[ï¼š:]\s*(.*)$", line)
        m = re.match(r"^\s*(ãƒ¡ãƒ¼ã‚«ãƒ¼å|ãƒ¡ãƒ¼ã‚«ãƒ¼|ï¾’ï½°ï½¶ï½°)[ï¼š:]\s*(.*)$", line)
        r = re.match(r"^\s*(ç†ç”±|è£œè¶³|æ ¹æ‹ )[ï¼š:]\s*(.*)$", line)
        if b: brand = b.group(2).strip()
        elif m: maker = m.group(2).strip()
        elif r: reason = r.group(2).strip()
    return brand, maker, reason

# === UI: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‹ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
uploaded_file = st.file_uploader("ğŸ“„ AIè£œå®Œå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆCSVï¼‰", type=["csv"])

if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ"):
    st.session_state.pop("result_df", None)
    st.session_state.pop("error_df", None)
    st.experimental_rerun()

# === åˆå›å‡¦ç† or ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«çµæœãŒã‚ã‚Œã°è¡¨ç¤º
if uploaded_file and "result_df" not in st.session_state:
    df = pd.read_csv(uploaded_file, dtype=str).fillna("")
    st.write("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿", df)

    required = ["ãƒ¦ãƒ‹ãƒ¼ã‚¯å", "å‹ç•ª", "JANã‚³ãƒ¼ãƒ‰"]
    if not all(c in df.columns for c in required):
        st.error(f"å¿…è¦åˆ—ãŒä¸è¶³: {set(required) - set(df.columns)}")
        st.stop()

    exclude_list = load_exclude_list_from_github()

    b_list, m_list, r_list, q_list, s_list, u_list, err_rows = [], [], [], [], [], [], []
    progress = st.progress(0)
    status = st.empty()

    for i, row in df.iterrows():
        status.text(f"{i+1}/{len(df)} å‡¦ç†ä¸­â€¦")
        progress.progress((i + 1) / len(df))

        query_parts = [
            get_safe(row, 'ãƒ¦ãƒ‹ãƒ¼ã‚¯å'), get_safe(row, 'å‹ç•ª'), get_safe(row, 'JANã‚³ãƒ¼ãƒ‰'),
            get_safe(row, 'ç®¡ç†ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤§å¤§'), get_safe(row, 'ç®¡ç†ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤§'),
            get_safe(row, 'ç®¡ç†ã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸­'), get_safe(row, 'ç®¡ç†ã‚«ãƒ†ã‚´ãƒªãƒ¼å°'),
            get_safe(row, 'å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤§å¤§'), get_safe(row, 'å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤§'),
            get_safe(row, 'å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸­'), get_safe(row, 'å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼å°')
        ]
        query = " ".join([q for q in query_parts if q.strip()])
        q_list.append(query)

        try:
            summary, urls = google_search(query, exclude_list)
            prompt = f"""
ä»¥ä¸‹ã¯å•†å“ã€Œ{get_safe(row, 'ãƒ¦ãƒ‹ãƒ¼ã‚¯å')}ã€ã«é–¢ã™ã‚‹Webæ¤œç´¢çµæœã®è¦ç´„ã§ã™ï¼š

{summary}

ã“ã®æƒ…å ±ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰åã¨ãƒ¡ãƒ¼ã‚«ãƒ¼åã‚’ä»¥ä¸‹ã®å½¢å¼ã§ç­”ãˆã¦ãã ã•ã„ï¼š

ãƒ–ãƒ©ãƒ³ãƒ‰ï¼š
ãƒ¡ãƒ¼ã‚«ãƒ¼ï¼š
ç†ç”±ï¼š
            """
            res = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å•†å“åˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚åˆ¤æ–­ãŒå›°é›£ãªå ´åˆã¯ç©ºæ¬„ã§æ§‹ã„ã¾ã›ã‚“ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            content = res.choices[0].message.content
            brand, maker, reason = parse_gpt_output(content)
        except Exception as e:
            brand, maker, reason = "", "", ""
            err_rows.append(row.to_dict())
            summary, urls = "", ""

        b_list.append(brand)
        m_list.append(maker)
        r_list.append(reason)
        s_list.append(summary[:300])
        u_list.append(urls)

    df["æ¤œç´¢ã‚¯ã‚¨ãƒª"] = q_list
    df["AI_ãƒ–ãƒ©ãƒ³ãƒ‰"] = b_list
    df["AI_ãƒ¡ãƒ¼ã‚«ãƒ¼"] = m_list
    df["AI_ç†ç”±"] = r_list
    df["æ¤œç´¢ã‚µãƒãƒª"] = s_list
    df["å‚ç…§URL"] = u_list

    st.session_state.result_df = df
    st.session_state.error_df = pd.DataFrame(err_rows)

# === å‡ºåŠ›ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
if "result_df" in st.session_state:
    st.download_button(
        "ğŸ“¥ è£œå®ŒçµæœCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        st.session_state.result_df.to_csv(index=False).encode("utf-8-sig"),
        file_name="AIè£œå®Œçµæœ_çµ±åˆç‰ˆ.csv",
        mime="text/csv"
    )

if "error_df" in st.session_state and not st.session_state.error_df.empty:
    st.warning(f"âš ï¸ ã‚¨ãƒ©ãƒ¼è¡Œæ•°: {len(st.session_state.error_df)} ä»¶")
    st.download_button(
        "âš ï¸ ã‚¨ãƒ©ãƒ¼è¡ŒCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        st.session_state.error_df.to_csv(index=False).encode("utf-8-sig"),
        file_name="AIè£œå®Œã‚¨ãƒ©ãƒ¼è¡Œ.csv",
        mime="text/csv"
    )
