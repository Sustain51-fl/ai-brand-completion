import streamlit as st
import pandas as pd
import openai
import requests
import base64
import re
import uuid
from datetime import datetime
import io
import time
import os
import tempfile

# === Secrets ===
openai.api_key = st.secrets["openai_api_key"]
GOOGLE_API_KEY = st.secrets["google_api_key"]
GOOGLE_CX = st.secrets["google_cse_id"]
GITHUB_TOKEN = st.secrets["github_token"]
GITHUB_REPO = st.secrets["github_repo"]
EXCLUDE_PATH = st.secrets["exclude_path"]

client = openai.OpenAI(api_key=openai.api_key)
st.set_page_config(layout="wide")
st.title("ğŸ§  ãƒ¡ãƒ¼ã‚«ãƒ¼ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰è£œå®Œãƒ„ãƒ¼ãƒ«ï¼ˆã‚³ãƒ¼ãƒ‰æ•´åˆæ€§å¯¾å¿œï¼‰")

# === è£œåŠ©é–¢æ•° ===
def generate_unique_filename(base: str, ext: str = "csv"):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    uid = uuid.uuid4().hex[:8]
    return f"{base}_{ts}_{uid}.{ext}"

def create_temp_logfile():
    tmp_dir = tempfile.gettempdir()
    fname = generate_unique_filename("ai_completion_log", "txt")
    return os.path.join(tmp_dir, fname)

if "logs" not in st.session_state:
    st.session_state.logs = []

def log(msg):
    stamp = datetime.now().strftime("[%H:%M:%S]")
    st.session_state.logs.append(f"{stamp} {msg}")

def call_gpt_with_retry(prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            res = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": (
                        "ã‚ãªãŸã¯å•†å“åˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€ãƒ–ãƒ©ãƒ³ãƒ‰åã¨ãƒ¡ãƒ¼ã‚«ãƒ¼åã‚’æ­£ã—ãæ¨å®šã—ã¦ãã ã•ã„ï¼š\n"
                        "- ãƒ–ãƒ©ãƒ³ãƒ‰ã¯è£½å“åã‚„ã‚µãƒ¼ãƒ“ã‚¹åã‚’æŒ‡ã—ã€å¿…ãšã„ãšã‚Œã‹ã®ãƒ¡ãƒ¼ã‚«ãƒ¼ã«å±ã—ã¦ã„ã‚‹ã€‚\n"
                        "- ãƒ¡ãƒ¼ã‚«ãƒ¼ã¯è£½é€ ãƒ»è²©å£²å…ƒã§ã‚ã‚Šã€ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’ä¿æœ‰ã™ã‚‹ä¼æ¥­åã§ã™ã€‚\n"
                        "- è‰²åãƒ»ã‚«ãƒ†ã‚´ãƒªãƒ»ç´ æï¼ˆä¾‹ï¼šãƒ›ãƒ¯ã‚¤ãƒˆã€ç´™ã€æ´—å‰¤ç­‰ï¼‰ã¯ãƒ–ãƒ©ãƒ³ãƒ‰ã¨ã¿ãªã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
                        "- ãƒ–ãƒ©ãƒ³ãƒ‰ãŒä¸æ˜ãªå ´åˆã¯ã€ãƒ–ãƒ©ãƒ³ãƒ‰ï¼šè©²å½“ãªã—ã€ã¨ã—ã¦ãã ã•ã„ã€‚\n"
                        "ãƒ–ãƒ©ãƒ³ãƒ‰ã¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã¯æ­£ç¢ºã«å¯¾å¿œã•ã›ã¦ãã ã•ã„ã€‚"
                    )},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            return res.choices[0].message.content
        except Exception as e:
            log(f"GPTãƒªãƒˆãƒ©ã‚¤{attempt+1}å›ç›®å¤±æ•—: {e}")
            time.sleep(2 ** attempt)
    raise RuntimeError("GPT APIå‘¼ã³å‡ºã—å¤±æ•—")

def google_search_with_retry(query, exclude_domains, max_retries=3):
    for attempt in range(max_retries):
        try:
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                "key": GOOGLE_API_KEY,
                "cx": GOOGLE_CX,
                "q": query,
                "num": 5,
                "lr": "lang_ja"
            }
            res = requests.get(url, params=params, timeout=15)
            res.raise_for_status()
            data = res.json()
            results, urls = [], []
            for item in data.get("items", []):
                link = item.get("link", "")
                if any(x in link for x in exclude_domains):
                    continue
                title = item.get("title", "")
                snippet = item.get("snippet", "")
                results.append(f"{title}\n{snippet}\n{link}")
                urls.append(link)
            return "\n\n".join(results), ", ".join(urls[:3])
        except Exception as e:
            log(f"Googleæ¤œç´¢ãƒªãƒˆãƒ©ã‚¤{attempt+1}å›ç›®å¤±æ•—: {e}")
            time.sleep(2 ** attempt)
    return "[GoogleSearchError] å…¨ãƒªãƒˆãƒ©ã‚¤å¤±æ•—", ""
# === GitHubé™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³èª­ã¿è¾¼ã¿ ===
@st.cache_data
def load_exclude_list_from_github():
    url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/main/{EXCLUDE_PATH}"
    try:
        df = pd.read_csv(url)
        return df.iloc[:, 0].dropna().tolist()
    except Exception as e:
        st.warning(f"âŒ é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def upload_to_github(content_str: str):
    get_url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{EXCLUDE_PATH}"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json"
    }
    sha = None
    try:
        res = requests.get(get_url, headers=headers)
        if res.status_code == 200:
            sha = res.json().get("sha")
    except:
        pass

    encoded = base64.b64encode(content_str.encode("utf-8")).decode("utf-8")
    payload = {
        "message": "æ›´æ–°: é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆ",
        "content": encoded,
        "branch": "main"
    }
    if sha:
        payload["sha"] = sha

    put_res = requests.put(get_url, headers=headers, json=payload)
    return put_res.status_code, put_res.json()

# === ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šé™¤å¤–ãƒªã‚¹ãƒˆç®¡ç† ===
st.sidebar.header("ğŸ›¡ï¸ é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ç®¡ç†")
exclude_list = load_exclude_list_from_github()
st.sidebar.code("\n".join(exclude_list) or "ï¼ˆé™¤å¤–ãƒªã‚¹ãƒˆãªã—ï¼‰")

uploaded_exclude = st.sidebar.file_uploader("ğŸ“¤ é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆï¼ˆCSVï¼‰", type=["csv"])
if uploaded_exclude:
    content = uploaded_exclude.getvalue().decode("utf-8")
    if st.sidebar.button("ğŸš€ GitHubã¸ä¿å­˜"):
        status, resp = upload_to_github(content)
        if status in (200, 201):
            st.sidebar.success("âœ… ä¿å­˜æˆåŠŸã€‚ãƒªãƒ­ãƒ¼ãƒ‰ã§åæ˜ ã•ã‚Œã¾ã™")
            st.cache_data.clear()
        else:
            st.sidebar.error(f"âŒ ä¿å­˜å¤±æ•—: {resp}")

# === ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚³ãƒ¼ãƒ‰ä»˜ããƒã‚¹ã‚¿èª­è¾¼ ===
st.sidebar.header("ğŸ“š ã‚³ãƒ¼ãƒ‰ä»˜ããƒã‚¹ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
uploaded_master = st.sidebar.file_uploader("ãƒ–ãƒ©ãƒ³ãƒ‰ãƒã‚¹ã‚¿ï¼ˆCSVï¼‰", type=["csv"])
brand_dict = {}
if uploaded_master:
    try:
        df_master = pd.read_csv(uploaded_master, dtype=str).fillna("")
        valid_rows = 0
        for _, row in df_master.iterrows():
            brand = row.get("ãƒ–ãƒ©ãƒ³ãƒ‰å", "").strip()
            if not brand:
                continue
            bcd = row.get("ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰", "").strip()
            mcd = row.get("ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚³ãƒ¼ãƒ‰", "").strip()
            mkr = row.get("ãƒ¡ãƒ¼ã‚«ãƒ¼å", "").strip()
            brand_dict[brand] = (bcd, mcd, mkr)
            valid_rows += 1
        st.sidebar.success(f"âœ… èª­ã¿è¾¼ã¿æˆåŠŸï¼ˆæœ‰åŠ¹ãƒ–ãƒ©ãƒ³ãƒ‰æ•°: {valid_rows}ï¼‰")
    except Exception as e:
        st.sidebar.error(f"âŒ ãƒã‚¹ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
def get_safe(row, col):
    return row[col] if col in row and pd.notnull(row[col]) else ""

def parse_gpt_output(text):
    brand, maker, reason = "", "", ""
    for line in text.splitlines():
        b = re.match(r"^\s*(ãƒ–ãƒ©ãƒ³ãƒ‰å|ãƒ–ãƒ©ãƒ³ãƒ‰|ï¾Œï¾ï¾—ï¾ï¾„ï¾)[ï¼š:]\s*(.*)$", line)
        m = re.match(r"^\s*(ãƒ¡ãƒ¼ã‚«ãƒ¼å|ãƒ¡ãƒ¼ã‚«ãƒ¼|ï¾’ï½°ï½¶ï½°)[ï¼š:]\s*(.*)$", line)
        r = re.match(r"^\s*(ç†ç”±|è£œè¶³|æ ¹æ‹ )[ï¼š:]\s*(.*)$", line)
        if b: brand = b.group(2).strip()
        elif m: maker = m.group(2).strip()
        elif r: reason = r.group(2).strip()
    return brand, maker, reason

uploaded_file = st.file_uploader("ğŸ“„ AIè£œå®Œå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆCSVï¼‰", type=["csv"])

if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ"):
    st.session_state.pop("result_df", None)
    st.session_state.pop("error_df", None)
    st.experimental_rerun()

if uploaded_file and "result_df" not in st.session_state:

    if not brand_dict:
        st.error("âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»ãƒ¡ãƒ¼ã‚«ãƒ¼ã®ãƒã‚¹ã‚¿CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    df = pd.read_csv(uploaded_file, dtype=str).fillna("")
    st.write("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿", df)

    required = ["ãƒ¦ãƒ‹ãƒ¼ã‚¯å", "å‹ç•ª", "JANã‚³ãƒ¼ãƒ‰"]
    if not all(c in df.columns for c in required):
        st.error(f"å¿…è¦åˆ—ãŒä¸è¶³: {set(required) - set(df.columns)}")
        st.stop()

    b_list, m_list, r_list, q_list, s_list, u_list = [], [], [], [], [], []
    bcd_list, mcd_list, mmk_list, match_flag_list = [], [], [], []
    err_rows = []

    progress = st.progress(0)
    status = st.empty()

    for i, row in df.iterrows():
        status.text(f"{i+1}/{len(df)} å‡¦ç†ä¸­â€¦")
        progress.progress((i + 1) / len(df))

        query_parts = [
            get_safe(row, 'ãƒ¦ãƒ‹ãƒ¼ã‚¯å'), get_safe(row, 'å‹ç•ª'), get_safe(row, 'JANã‚³ãƒ¼ãƒ‰'),
            get_safe(row, 'ç®¡ç†ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤§å¤§'), get_safe(row, 'ç®¡ç†ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤§'),
            get_safe(row, 'ç®¡ç†ã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸­'), get_safe(row, 'ç®¡ç†ã‚«ãƒ†ã‚´ãƒªãƒ¼å°'),
            get_safe(row, 'å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤§å¤§'), get_safe(row, 'å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤§'),
            get_safe(row, 'å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸­'), get_safe(row, 'å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ¼å°')
        ]
        query = " ".join([q for q in query_parts if q.strip()])
        q_list.append(query)

        log(f"å‡¦ç†é–‹å§‹: {query}")

        try:
            summary, urls = google_search_with_retry(query, exclude_list)
            prompt = f"""
ä»¥ä¸‹ã¯å•†å“ã€Œ{get_safe(row, 'ãƒ¦ãƒ‹ãƒ¼ã‚¯å')}ã€ã«é–¢ã™ã‚‹Webæ¤œç´¢çµæœã®è¦ç´„ã§ã™ï¼š

{summary}

ã“ã®æƒ…å ±ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰åã¨ãƒ¡ãƒ¼ã‚«ãƒ¼åã‚’ä»¥ä¸‹ã®å½¢å¼ã§ç­”ãˆã¦ãã ã•ã„ï¼š

ãƒ–ãƒ©ãƒ³ãƒ‰ï¼š
ãƒ¡ãƒ¼ã‚«ãƒ¼ï¼š
ç†ç”±ï¼š
"""
            res_text = call_gpt_with_retry(prompt)
            log(f"â†’ GPTå›ç­”å…ˆé ­è¡Œ: {res_text.splitlines()[0] if res_text else 'ãªã—'}")
            brand, maker, reason = parse_gpt_output(res_text)
        except Exception as e:
            log(f"âŒ å‡¦ç†å¤±æ•—: {e}")
            brand, maker, reason = "", "", ""
            summary, urls = "", ""
            err_rows.append(row.to_dict())

        if brand in brand_dict:
            bcd, mcd, mmk = brand_dict[brand]
            match_flag = "ã€‡" if maker == mmk else "Ã—"
            mas_br = brand  # AIãŒå‡ºã—ãŸãƒ–ãƒ©ãƒ³ãƒ‰ãŒãƒã‚¹ã‚¿ã«å­˜åœ¨ã—ã¦ã„ã‚‹ã®ã§ã€ãã‚Œã‚’ãƒã‚¹ã‚¿ãƒ–ãƒ©ãƒ³ãƒ‰åã¨ã™ã‚‹
        else:
            bcd, mcd, mmk, match_flag, mas_br = "", "", "", "ï¼ˆãƒã‚¹ã‚¿ãªã—ï¼‰", ""

        b_list.append(brand)
        m_list.append(maker)
        r_list.append(reason)
        s_list.append(summary[:300])
        u_list.append(urls)
        bcd_list.append(bcd)
        mcd_list.append(mcd)
        mmk_list.append(mmk)
        match_flag_list.append(match_flag)
        mas_br_list.append(mas_br)  # â˜… è¿½åŠ 

    df["æ¤œç´¢ã‚¯ã‚¨ãƒª"] = q_list
    df["AI_ãƒ–ãƒ©ãƒ³ãƒ‰"] = b_list
    df["AI_ãƒ¡ãƒ¼ã‚«ãƒ¼"] = m_list
    df["AI_ç†ç”±"] = r_list
    df["æ¤œç´¢ã‚µãƒãƒª"] = s_list
    df["å‚ç…§URL"] = u_list
    df["ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰"] = bcd_list
    df["ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚³ãƒ¼ãƒ‰"] = mcd_list
    df["ãƒã‚¹ã‚¿ãƒ–ãƒ©ãƒ³ãƒ‰å"] = mas_br_list    # â˜… è¿½åŠ 
    df["ãƒã‚¹ã‚¿ãƒ¡ãƒ¼ã‚«ãƒ¼å"] = mmk_list
    df["ãƒ–ãƒ©ãƒ³ãƒ‰â‡„ãƒ¡ãƒ¼ã‚«ãƒ¼æ•´åˆæ€§"] = match_flag_list

    st.session_state.result_df = df
    st.session_state.error_df = pd.DataFrame(err_rows)

if "result_df" in st.session_state:
    st.markdown("### âœ… è£œå®Œçµæœ")
    st.dataframe(st.session_state.result_df, use_container_width=True)

    result_filename = generate_unique_filename("AIè£œå®Œçµæœ_ã‚³ãƒ¼ãƒ‰å¯¾å¿œ", "csv")
    st.download_button(
        "ğŸ“¥ è£œå®ŒçµæœCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        st.session_state.result_df.to_csv(index=False).encode("utf-8-sig"),
        file_name=result_filename,
        mime="text/csv"
    )

if "error_df" in st.session_state and not st.session_state.error_df.empty:
    st.markdown("### âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿè¡Œ")
    st.dataframe(st.session_state.error_df, use_container_width=True)

    error_filename = generate_unique_filename("AIè£œå®Œã‚¨ãƒ©ãƒ¼è¡Œ", "csv")
    st.download_button(
        "âš ï¸ ã‚¨ãƒ©ãƒ¼è¡ŒCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        st.session_state.error_df.to_csv(index=False).encode("utf-8-sig"),
        file_name=error_filename,
        mime="text/csv"
    )

# === ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å‡¦ç†ãƒ­ã‚°ã‚’è¡¨ç¤ºï¼ˆãƒ•ã‚©ãƒ³ãƒˆå°ãƒ»è‰²åˆ†ã‘ï¼‰ ===
if st.session_state.logs:
    st.sidebar.markdown("### ğŸ§¾ å‡¦ç†ãƒ­ã‚°")
    with st.sidebar.expander("ãƒ­ã‚°ã‚’è¡¨ç¤º"):
        for line in st.session_state.logs:
            if "âŒ" in line or "å¤±æ•—" in line:
                color = "red"
            elif "ã€‡" in line or "âœ…" in line:
                color = "green"
            else:
                color = "black"

            st.markdown(
                f"<div style='font-size:12px; color:{color}; font-family:monospace'>{line}</div>",
                unsafe_allow_html=True
            )

    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦DLãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
    log_file_path = create_temp_logfile()
    with open(log_file_path, "w", encoding="utf-8") as f:
        f.write("\n".join(st.session_state.logs))

    st.sidebar.download_button(
        "ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’DL",
        data=open(log_file_path, "rb").read(),
        file_name=os.path.basename(log_file_path),
        mime="text/plain"
    )
